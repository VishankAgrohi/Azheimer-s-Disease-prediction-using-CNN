# -*- coding: utf-8 -*-
"""Alzheimer's Disease Analysis and Predictions

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CZ_3JoacBcgEzeSlIfk0ERRN86cUzw9V

# **1. Introduction and Understanding the Dataset**

This dataset contains extensive health information for 2,149 patients, each uniquely identified with IDs ranging from 4751 to 6900. The dataset includes demographic details, lifestyle factors, medical history, clinical measurements, cognitive and functional assessments, symptoms, and a diagnosis of Alzheimer's Disease. It is ideal for researchers and data scientists looking to explore factors associated with Alzheimer's, develop predictive models, and conduct statistical analyses.

#### What's Alzheimer's Disease ?
Alzheimer's Disease is a brain illness that slowly destroys memory and thinking skills. It is the most common cause of dementia, which makes it hard for people to remember things, think clearly, and take care of themselves. The disease happens because of harmful changes in the brain, like the buildup of bad proteins. Symptoms typically begin with mild memory loss and confusion, eventually progressing to severe cognitive impairment and loss of the ability to perform daily activities. Risk factors include age, family history, genetics, lifestyle, and certain medical conditions. There is currently no cure, but treatments can help manage symptoms and improve quality of life.

#### Now lets understand each Columns:

**Demographic Details**
1. PatientID: A unique identifier assigned to each patient (4751 to 6900).
2. Age: The age of the patients ranges from 60 to 90 years.
3. Gender: Gender of the patients, where 0 represents Male and 1 represents Female.
4. Ethnicity: The ethnicity of the patients, coded as follows:
    0: Caucasian
    1: African American
    2: Asian
    3: Other
5. EducationLevel: The education level of the patients, coded as follows:
    0: None
    1: High School
    2: Bachelor's
    3: Higher

**Lifestyle Factors**
1. BMI: Body Mass Index of the patients, ranging from 15 to 40.
2. Smoking: Smoking status, where 0 indicates No and 1 indicates Yes.
3. AlcoholConsumption: Weekly alcohol consumption in units, ranging from 0 to 20.
4. PhysicalActivity: Weekly physical activity in hours, ranging from 0 to 10.
5. DietQuality: Diet quality score, ranging from 0 to 10.
6. SleepQuality: Sleep quality score, ranging from 4 to 10.

**Medical History**
1. FamilyHistoryAlzheimers: Family history of Alzheimer's Disease, where 0 indicates No and 1 indicates Yes.
2. CardiovascularDisease: Presence of cardiovascular disease, where 0 indicates No and 1 indicates Yes.
3. Diabetes: Presence of diabetes, where 0 indicates No and 1 indicates Yes.
4. Depression: Presence of depression, where 0 indicates No and 1 indicates Yes.
5. HeadInjury: History of head injury, where 0 indicates No and 1 indicates Yes.
6. Hypertension: Presence of hypertension, where 0 indicates No and 1 indicates Yes.

**Clinical Measurements**
1. SystolicBP: Systolic blood pressure, ranging from 90 to 180 mmHg.
2. DiastolicBP: Diastolic blood pressure, ranging from 60 to 120 mmHg.
3. CholesterolTotal: Total cholesterol levels, ranging from 150 to 300 mg/dL.
4. CholesterolLDL: Low-density lipoprotein cholesterol levels, ranging from 50 to 200 mg/dL.
5. CholesterolHDL: High-density lipoprotein cholesterol levels, ranging from 20 to 100 mg/dL.
6. CholesterolTriglycerides: Triglycerides levels, ranging from 50 to 400 mg/dL.

**Cognitive and Functional Assessments**
1. MMSE: Mini-Mental State Examination score, ranging from 0 to 30. Lower scores indicate cognitive impairment.
2. FunctionalAssessment: Functional assessment score, ranging from 0 to 10. Lower scores indicate greater impairment.
3. MemoryComplaints: Presence of memory complaints, where 0 indicates No and 1 indicates Yes.
4. BehavioralProblems: Presence of behavioral problems, where 0 indicates No and 1 indicates Yes.
5. ADL: Activities of Daily Living score, ranging from 0 to 10. Lower scores indicate greater impairment.

**Symptoms**
1. Confusion: Presence of confusion, where 0 indicates No and 1 indicates Yes.
2. Disorientation: Presence of disorientation, where 0 indicates No and 1 indicates Yes.
3. PersonalityChanges: Presence of personality changes, where 0 indicates No and 1 indicates Yes.
4. DifficultyCompletingTasks: Presence of difficulty completing tasks, where 0 indicates No and 1 indicates Yes.
5. Forgetfulness: Presence of forgetfulness, where 0 indicates No and 1 indicates Yes.

**Target Variable**
* Diagnosis: Diagnosis status for Alzheimer's Disease, where 0 indicates No and 1 indicates Yes.

**Confidential Information**
1. DoctorInCharge: This column contains confidential information about the doctor in charge, with "XXXConfid" as the value for all patients.

# **2. Importing all Essential Dependencies**
"""

!pip install catboost

import pandas as pd
import numpy as np

# Visualization
import seaborn as sns
import plotly.express as px
import matplotlib.pyplot as plt

# Machine Learning Models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

# Ensemble Models
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostClassifier
from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier

# Tensorflow and Keras for Neural Network
import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, Flatten, Dropout, Input

# Useful
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler, PowerTransformer
from sklearn.impute import SimpleImputer
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from imblearn.pipeline import Pipeline as Imbpipeline
from imblearn.over_sampling import SMOTE

# Ignore all warnings
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)

"""# **3. Loading Dataset into DataFrame**"""

df = pd.read_csv("/content/alzheimers_disease_data (1).csv")
df

"""# **4. Exploratory Data Analysis (EDA)**

We will first check if there's the need of Data Cleaning and Data Preprocessing by checking the shape, Null or missing values, duplicates, Outliers, Imbalance, etc then we will do proper Information extraction by doing Visualization using Matplotlib, Seboarn and Plotly

## A. Quick Analysis
"""

pd.set_option('display.max_columns', 99)

df.head()

df.shape

"""So the Dataset is pretty small, however it has alot of Columns!

### Checking Null or Missing values
"""

df.isna().sum()

df.info()

"""Okay so no any Null Values, that saves us alot of time.

Now, lets check Duplicates

### Checking Duplicates values

Lets check if there's any Duplicated or Repeated entries for the same Patient, we will check using the "PatientID" column.
"""

df['PatientID'].duplicated().sum()

"""Okay, so there's no Duplicates which is good, now since in Machine Learning we dont really have much use of ID's, we can simply drop this column, wont be a problem."""

df.drop(['PatientID'], axis=1, inplace=True)
df.shape

"""### Check Column DataTypes"""

df.dtypes

df.head()

"""By Analysing each column types and format above we can see that all of the important columns are already in Numerical format (Except DoctorInCharge), So we wont need to do any Encoding, that saves alot of time.

However, we will need to divide the Numerical columns into **Non Binary** and **Binary** so According to that we can apply Scaling later.

Now, lets investigate the ONLY text column **DoctorInCharge** and see whats going on with it.
"""

df['DoctorInCharge'].duplicated().sum()

"""So the **DoctorInCharge** have the same value for all the rows meaning its useless, and it doesn't even contribute any information to the diagnosis as well, so i will just drop it."""

df.drop(['DoctorInCharge'], axis=1, inplace=True)
df.shape

df.dtypes

"""Alright, Now we only have Numeric columns.

Now, lets get all the Binary Numeric columns and store them into a variable for further uses.
"""

binary_col = []
for col in df.columns:
    if df[col].dtype in ["int64", "float64"]:
        unique_val = df[col].nunique()
        if unique_val == 2:
            binary_col.append(col)
df[binary_col]

"""We will do the same to get all Non Binary columns"""

nonbinary_col = []
for col in df.columns:
    if df[col].dtype in ["int64", "float64"]:
        unique_val = df[col].nunique()
        if unique_val > 2:
            nonbinary_col.append(col)
df[nonbinary_col]

"""So we dont need to do any sort of Preprocessing for the Binary columns, but we do need to do some preprocessing later on the Non Binary columns.

Now, lets check Imbalance in the dataset as its important, we dont want an Imbalance in the dataset while building ML Models

### Check Imbalance
"""

df['Diagnosis'].value_counts()

1389 / 760

"""So the Imbalance ratio is 1.82:1 indicating that there are about 1.82 times more samples in the majority class than in the minority class.

To assess the imbalance, the following general guidelines can help:

* 1:1: Perfectly balanced.
* 1:2 to 1:4: Mild imbalance.
* 1:4 to 1:10: Moderate imbalance.
* 1:10+: Severe imbalance.

So our Dataset doesn't have a Severe Imbalance but still something needed to be handled.

NOTE: I will Handle the Imabalance in the dataset after Train Test split, that way there wont be any data leakage for Training or Testing set which can happen if you Oversample before doing Splitting

### Checking Outliers

lets Analyze the above Non-Binary Numeric Columns and see if there's any Outliers, we can do this simply by using the pandas describe() method first, which if you have some Domain knowledge and a Common sense you could tell if there's Outliers or not, else if you are not sure you can use Outliers Detection rules such as **3 Sigma rule**, **IQR rule** or the **Percentage Range rule**
"""

df[nonbinary_col].describe()

"""So upon analyzing, We can see there doesn't seem to be any Outliers, everything seems pretty neat, so we dont need to do any further Outliers detection since this is clear enough.

**From above table, We can come to a conclusion of our quick analysis:**

* Average age is 74
* 75% of patients are African American.
* Average BMI is 27-28
* On average patients have less Physical activity and bad Diet quality
* Average patients SystolicBP is 134 which indicates Hypertension stage 1
* Average patients DiastolicBP is 89-90 which indicates Elavated-High Hypertension
* Average Total Cholesterol is 225 which indicates Borderline High risk for cardiovascular diseases
* Average CholesterolLDL is 124 which indicates Near Optimal/Above Optimal risk of Heart disease
* Average CholesterolHDL is 59-60 which indicates Near high lvl which is okay
* Average CholesterolTriglycerides is 228 indicates High risk of Heart attack
* Average MMSE Score is 14-15 which indicates Moderate Cognitive Impairment
* Average FunctionalAssessment Score is 5 which indicates Moderate Impairment
* Average ADL Score is 4-5 which indicates Moderate Impairment

## B. Visualization and Deep Analysis

Now, lets get to Visualization to get further and in-dept Understanding of each columns, we will be checking Correlations, Distributions, etc for the Columns and come up with some Conclusions.

Note: I'm still learning, so im still not that good at Visualization but i try my best to and find an end Conclusion.

### Check Normality

First, I want to check the Normality of each columns using **Shapiro-Wilk Test**, I will store the Columns with Non-Normal Distribution into a seperate variable so we can maybe later apply **PowerTransformer** to it
"""

from scipy.stats import shapiro
non_normal_columns = []
for column in df.columns:
    stat, p = shapiro(df[column].dropna())
    print(f'Shapiro-Wilk Test for {column}: Statistics={stat}, p={p}')
    if p < 0.05:
        non_normal_columns.append(column)
        print(f'{column} is not normally distributed (p < 0.05)')

non_normal_columns

len(non_normal_columns)

"""All of our columns are not normally distributed, we definately gonna have to apply **Power Scaling** to it during Preprocessing later.

### Check Distribution

Anyways, lets just get into Plotting, we will plot a Histplot for all of the Columns and Analyze their distributions, I can simply use a For loop and plot through all the Columns at once but i want to walk through each columns, so i will be plotting one by one and im creating a new DataFrame for only Diagnosis patients, where i will do the plotting on.
"""

diagnosis_df = df[df['Diagnosis']==1]
def plot(column, dataframe=diagnosis_df):
    plt.figure(figsize=(8, 4))
    sns.histplot(dataframe[column], kde=True)
    plt.title(f'Distribution of {column}')
    plt.show()

"""I will also create a function to plot both diagnosed vs not side by side just in case we might need it"""

def diagnosed_vs_not(column, dataframe):
    # Separate the diagnosed and non-diagnosed dataframes
    diagnosed_df = dataframe[dataframe['Diagnosis'] == 1]
    non_diagnosed_df = dataframe[dataframe['Diagnosis'] == 0]

    # Create a figure with two subplots
    fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)

    # Plot for diagnosed patients
    sns.histplot(diagnosed_df[column], kde=True, ax=axes[0])
    axes[0].set_title(f'Distribution of {column} (Diagnosed)')

    # Plot for non-diagnosed patients
    sns.histplot(non_diagnosed_df[column], kde=True, ax=axes[1])
    axes[1].set_title(f'Distribution of {column} (Non-Diagnosed)')

    plt.tight_layout()
    plt.show()

plot("Age")
print(f"Average age of Alzheimer's Disease Patients is: {np.mean(df[df['Diagnosis']==1]['Age'])}")

"""Well the Age column have a Uniform Distribution, we already saw above while checking the Normality of each columns that non of the columns have a Normal distirbution, so im assuming most of them are Uniform distribution, and the thing about Uniform distribution is its not very informative, but anyways, Average age of Patience who have Parkinson's Disease is 74 years old, which make sense because Alzheimer's Disease usually develops in middle or late life, with the average age of diagnosis being 70-75 years old and by age 85, the risk reaches nearly 50%."""

plot("Gender")

plot("Ethnicity")

"""Okay, so most Patients with Alzheimer's Disease are likely to be Caucasian, well it could also be maybe because in our Dataset there are more Caucasian as Caucasians are the most studied group in Alzheimer's research, lets check.."""

df['Ethnicity'].value_counts()

"""Yh, i was right, hmm, so lets do one thing, we will create seperate Dataframe for each Ethnicity and calculate the Percentage of each of them who are Diagnosed with the disease and see how many percentage have the disease"""

# Create separate DataFrames for each ethnicity
cau = df[df['Ethnicity'] == 0]
afr_am = df[df['Ethnicity'] == 1]
asian = df[df['Ethnicity'] == 2]
other = df[df['Ethnicity'] == 3]

# Calculate the percentage of patients diagnosed with Alzheimer's Disease in each ethnicity group
def calculate_percentage(df):
    diagnosed = df[df['Diagnosis'] == 1]
    percentage = (len(diagnosed) / len(df)) * 100
    return percentage


cau_percentage = calculate_percentage(cau)
afr_am_percentage = calculate_percentage(afr_am)
asian_percentage = calculate_percentage(asian)
other_percentage = calculate_percentage(other)

# Create a DataFrame for plotting
data = {
    'Ethnicity': ['Caucasian', 'African American', 'Asian', 'Other'],
    'Percentage Diagnosed': [cau_percentage, afr_am_percentage, asian_percentage, other_percentage]
}
plot_df = pd.DataFrame(data)

# Plot a pie chart using Plotly
fig = px.pie(plot_df, names='Ethnicity', values='Percentage Diagnosed', title='Percentage of Alzheimer\'s Disease Diagnoses by Ethnicity')
fig.show()

"""Welp, we can see a different story from the above pie chart, but yh its more of equally distributed"""

diagnosed_vs_not("EducationLevel", df)

"""### **Lifestyle Factors Visualization**"""

plot("BMI")
print(f"Average BMI of Alzheimer's Disease Patients is: {np.mean(df[df['Diagnosis']==1]['BMI'])}")

"""Okay, so BMI stands for Body Mass Index. It's a measure that uses a person's height and weight to estimate whether they are underweight, normal weight, overweight, or obese.

It categorizes individuals into different weight status categories:

* Underweight: BMI less than 18.5
* Normal weight: BMI 18.5 to 24.9
* Overweight: BMI 25 to 29.9
* Obesity: BMI 30 or higher

Although our BMI column have kind of a Uniform Distribution, we can still see the Average BMI of Alzheimer's Disease Patients is 27, But lets Categorize our BMI column into 4 categories for Patients with Alzheimer's Disease and plot a Pie Chart
"""

def categorize_bmi(bmi):
    if bmi < 18.5:
        return 'Underweight'
    elif 18.5 <= bmi <= 24.9:
        return 'Normal weight'
    elif 25 <= bmi <= 29.9:
        return 'Overweight'
    else:
        return 'Obesity'

# Apply the function to create a new column
df['BMI_Category'] = diagnosis_df['BMI'].apply(categorize_bmi)

# Count the occurrences of each category
bmi_counts = df['BMI_Category'].value_counts().reset_index()
bmi_counts.columns = ['BMI_Category', 'Count']

# Plot the pie chart using Plotly
fig = px.pie(bmi_counts, values='Count', names='BMI_Category', title='BMI Categories Distribution')
fig.show()

df.drop(['BMI_Category'], axis=1, inplace=True)

"""According to the Obesity Action Coalition, adults with a BMI of 25.0–29.9 in mid-life (ages 45–55) have a higher chance of developing Alzheimer's disease or dementia later in life (ages 65 and older). Adults with a BMI of more than 30.0 in mid-life have an even higher risk of developing Alzheimer's disease and vascular dementia later in life, and we can see on average the Patients who are Diagnosed are between the 25-29 range with 40% of them being in the Obese category"""

plot("Smoking")

"""Smoking is a risk factor for Alzheimer's disease (AD) and other types of dementia. The World Health Organization (WHO) estimates that smoking is responsible for 14% of AD cases worldwide. Smoking can increase the risk of AD in a number of ways.

We can see that more people dont smoke, but again this could be due to Imbalance in the dataset, lets analyze further
"""

df['Smoking'].value_counts()

# Create separate DataFrames for smokers and non-smokers
smoke = df[df['Smoking'] == 1]
no_smoke = df[df['Smoking'] == 0]

# Calculate the percentage of patients diagnosed with Alzheimer's Disease for smokers and non-smokers
smoke_per = calculate_percentage(smoke)
nosmoke_per = calculate_percentage(no_smoke)

# Create a DataFrame for plotting
data = {
    'Smoking': ['Non-Smoker', 'Smoker'],
    'Percentage Diagnosed': [nosmoke_per, smoke_per]
}
plot_df = pd.DataFrame(data)

# Plot a pie chart using Plotly
fig = px.pie(plot_df, names='Smoking', values='Percentage Diagnosed', title='Percentage of Alzheimer\'s Disease Diagnoses by Smoking Status')
fig.show()

"""So its 50-50 here"""

plot("AlcoholConsumption")

"""By looking at the graph we can see its a Uniform distribution again.

There is no consensus on the impact of alcohol on Alzheimer's disease (AD), The relationship between alcohol and Alzheimer's disease is complicated but some studies suggest that excessive drinking may increase the risk of dementia, and that heavy alcohol consumption may speed up cognitive decline in AD patients.

According to the Alzheimer's Association, alcohol use disorder is a risk factor for AD. Some studies suggest that alcohol may: Increase brain atrophy, Increase amyloid plaques, Cause brain injury, and Accelerate the loss of neurons and their connections.
Other studies suggest that alcohol may not impact AD, or that it may even decrease the risk. However, the validity and consistency of alcohol and AD measures across studies may be a limitation.

The National Institute for Health and Care Excellence (NICE) recommends reducing alcohol consumption as much as possible, especially in mid-life, to minimize the risk of developing age-related conditions like dementia
"""

plot("PhysicalActivity")
print(f"Average PhysicalActivity of Alzheimer's Disease Patients is: {np.mean(df[df['Diagnosis']==1]['PhysicalActivity'])}")

"""Low levels of physical activity are a risk factor associated with Alzheimer's disease. Older adults who exercise are more likely to maintain cognition and we can see that most Patients who are diagnosed have low physical activities"""

plot("DietQuality")
print(f"Average DietQuality of Alzheimer's Disease Patients is: {np.mean(df[df['Diagnosis']==1]['DietQuality'])}")

"""Diet quality is a critical factor in managing Alzheimer's Disease (AD) and potentially reducing the risk of developing it. Several dietary patterns and nutrients have been studied for their potential benefits in brain health and cognitive function.

Improving diet quality can be a crucial part of managing Alzheimer's Disease, supporting overall health, and potentially slowing the progression of cognitive decline.
"""

plot("SleepQuality")

"""Sleep quality is a crucial aspect of managing Alzheimer's Disease (AD) and can significantly impact cognitive function, overall health, and the progression of the disease.

Poor sleep quality and sleep disturbances are associated with impaired cognitive function. In Alzheimer's patients, inadequate sleep can exacerbate memory problems and other cognitive deficits. Sleep disturbances are common in AD and may contribute to the progression of the disease.

Poor sleep can worsen behavioral and psychological symptoms of dementia (BPSD), such as agitation, depression, and anxiety, leading to increased caregiver burden.

Improving sleep quality in Alzheimer's Disease patients requires a comprehensive approach that includes lifestyle modifications, environmental adjustments, and, when necessary, medical interventions. Good sleep hygiene practices and a supportive sleep environment can significantly enhance the quality of life for patients

### **Patient's Medical History Visualization**
"""

plot("FamilyHistoryAlzheimers")

"""Most of the Patients dont have any Family history of the Disease, Family history is not necessary for an individual to develop Alzheimer's. However, research shows that those who have a parent or sibling living with Alzheimer's are more likely to develop the disease than those who do not have a first-degree relative with Alzheimer's.


Lets do one thing, we will analyse the percentage of Patients having diagnosed with Alzheimer who has a family history of the disease
"""

def check_percent(col, df=df):
    col_name = df[df[col]==1]
    print(len(col_name[col_name['Diagnosis']==1]) / len(col_name) * 100)

check_percent("FamilyHistoryAlzheimers")

"""So yh, 32% Percent of patients who have a family history of the disease have also developed it"""

plot("CardiovascularDisease")

"""Most patients dont have any Cardiovascular Disease.

Cardiovascular disease (CVD) is a term for a range of conditions that affect the heart and blood vessels. Common types include coronary artery disease (which can lead to heart attacks), hypertension (high blood pressure), heart failure, and stroke. Essentially, it's any disease that affects the heart or blood vessels and can lead to serious health problems if not managed properly.

Cardiovascular disease is related to Alzheimer's disease because they share several risk factors and underlying mechanisms. Poor cardiovascular health, such as high blood pressure, high cholesterol, obesity, diabetes, and smoking, can increase the risk of developing Alzheimer's. The connection is primarily due to the impact that cardiovascular health has on brain health.

So lets look into the Patients who have Cardiovascular Disease and see how many of them are actually diagnosed with Alzheimer's as well
"""

check_percent("CardiovascularDisease")

"""So almost 40% of the patients with Cardiovascular Disease have developed Alzheimer's"""

plot("Diabetes")

"""So its same here, most diagnosed patients dont have Diabete.

Diabetes is a condition where the body either doesn't produce enough insulin or can't use insulin properly, leading to high blood sugar levels. This can cause damage to the body's organs, including the heart and blood vessels, increasing the risk of cardiovascular diseases. Poor blood sugar control can also affect the brain, leading to an increased risk of developing Alzheimer's disease and other forms of dementia, although i dont think Diabeets contribute as much, probably like 30%
"""

check_percent("Diabetes")

plot("Depression")

"""Depression is linked to an increased risk of developing Alzheimer's disease. While it's difficult to quantify exactly how much depression contributes to Alzheimer's on a specific scale, it is considered a significant risk factor. Depression can affect brain function and structure, potentially leading to changes that increase the likelihood of cognitive decline and dementia. Studies suggest that chronic depression, especially later in life, can double the risk of developing Alzheimer's disease"""

check_percent("Depression")

plot("HeadInjury")

"""People who have had a head injury are more likely to develop Alzheimer's disease. Head injuries, especially severe or repeated ones, can damage brain cells and lead to long-term changes in brain function. This damage can increase the risk of cognitive decline and dementia, including Alzheimer's disease. For example, traumatic brain injury (TBI) has been linked to a higher risk of Alzheimer's and other forms of dementia later in life."""

check_percent("HeadInjury")

plot("Hypertension")

"""Hypertension is also a significant risk factor for Alzheimer's disease. High blood pressure can lead to damage in the blood vessels of the brain, reducing blood flow and causing brain cell damage. This can contribute to cognitive decline and increase the risk of Alzheimer's and other forms of dementia."""

check_percent("Hypertension")

"""Okay so, i want to address here:

In our Dataset of Medical History columns, there are only about 30-39% patients with them even though we know that these factors are pretty important or contribute when it comes to Patients having the disease or not and this can affect the performance of a machine learning models, so we will need to do some Oversampling or Feature Engineering later to handle these issues, We will address them in the Data Preprocessing part, for now, we will ignore it.

### **Clinical Measurements Visualization**
"""

clinic_col = ["SystolicBP", "DiastolicBP", "CholesterolTotal", "CholesterolLDL", "CholesterolHDL", "CholesterolTriglycerides"]

for col in clinic_col:
    fig = px.histogram(df, x=col, title=f'Histogram of {col}',
                       color_discrete_sequence=['#636EFA'],
                       opacity=0.8)

    # Customize layout
    fig.update_layout(
        title_text=f'Histogram of {col}', # Title
        title_x=0.5,                      # Title position
        title_font=dict(size=20),         # Title font size
        xaxis_title_text=col,             # x-axis label
        yaxis_title_text='Count',         # y-axis label
        bargap=0.2,                       # Gap between bars
        template='plotly_dark',           # Dark theme
        paper_bgcolor='rgba(0,0,0,0)',    # Transparent background
        plot_bgcolor='rgba(0,0,0,0)',     # Transparent plot area
    )

    # Customize x and y axis
    fig.update_xaxes(showgrid=False, zeroline=False)
    fig.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='LightGrey')

    fig.show()

"""Since I'm not an expert about the above 6 Clinical Measurements, i can't say much about it without Digging in deeper into their research, but anyways, They all have kind of a Uniform Distirbution, which is not very informative, i might be wrong though!

### **Cognitive and Functional Assessments Visualization**
"""

plot("MMSE")

"""This is a skewed distribution and we can see most Diagnosed patients have lower MMSE Scores, Scores below 24 indicate some level of cognitive impairment, with lower scores showing more severe impairment, which is associated with Alzheimer's Disease, so this is an important feature."""

plot("FunctionalAssessment")

"""Most diagnosed patients have low scores below 4 which is bad as Lower scores indicate greater difficulty in performing daily activities, reflecting more severe functional impairment, often seen in Alzheimer's patients. This is also an Important feature"""

plot("MemoryComplaints")

check_percent("MemoryComplaints")

"""We can see a 63-65% Patients who have Presence of memory complaints also are Diagnosed with Alzheimer's Disease"""

plot("BehavioralProblems")

check_percent("BehavioralProblems")

"""Patients with Behavioral Problems are 60% more likely to be Diagnosed with Alzheimer"""

plot("ADL")

"""We can see most patients have lower ADL scores below 4 and we know that Lower scores indicate greater impairment.

So overall, the **Cognitive and Functional Assessments** columns are pretty useful and seems to contribute in determining whether a person have Alzheimer or not

### **Symptoms**
"""

symptoms = ["Confusion", "Disorientation", "PersonalityChanges", "DifficultyCompletingTasks", "Forgetfulness"]
for col in symptoms:
    fig = px.pie(values=df[col].value_counts(), names=df[col].value_counts().index, title=f"Percentage of {col}", hole=0.2)
    fig.update_traces(textinfo='percent+label')
    fig.show()

"""The above features play a role in determining Alzheimer's disease, but we can see most diagnosed patients of 70-85% dont have these so it might suggest that the dataset includes patients in various stages of the disease, or that these symptoms are not uniformly reported or recorded. Early-stage Alzheimer's might present with less pronounced symptoms, or the dataset might be missing some data.

Anyways, we will see if we can do some Feature Engineering as well for these later in the Preprocessing step.

### **Correlation Heatmap**
"""

plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(), annot=False, cmap="coolwarm", fmt=".2f", linewidths=0.5, linecolor="pink")
plt.title("Correlation Matrix")
plt.show()

"""We can see that

There are some strong Correlation between our Target variable with the **Cognitive and Functional Assessments** columns:

Negative correlation with MMSE, FunctionalAssesment and ADL.

Slight positive correlation with MemoryComplaints and BehavioralProblems.

### **Correlation with the target variable**
"""

corr_with_target = df.corr()['Diagnosis'].sort_values(ascending=False)
plt.figure(figsize=(10, 6))
sns.barplot(x=corr_with_target.index, y=corr_with_target.values)
plt.xticks(rotation=90)
plt.title('Correlation of Features with Diagnosis')
plt.show()

"""Well, i think thats enough of Visualization and Analysis, now lets come to some end Conclusions"""

df.shape

"""## **C. CONCLUSIONS of EDA**

**Machine Learning**
* The Dataset is pretty small with over 2k datas but 33 (Useful) Features
* No Data Cleaning is required since there is no Missing values, Duplicates, Outliers nor any Wrong formatted values
* There are no Text Categorical columns meaning no Encoding is needed either
* There are Non-Binary Numerical columns which needed to be Scaled
* There is a Moderate Imbalance in the dataset which need to be handled
* Most of the columns are more of a Uniform Distribution rather than Normal (Gaussian) Distribution
* Feature Engineering is needed for Medical History columns as they are more impactful in predicting the disease
* Cognitive and Functional Assessments columns scores are important features for predicting the disease
* See if we can do some Feature Engineering on Symptoms columns to increase their contribution in predicting the disease


**Analysis Conclusions**
* Alzheimer's Disease usually develops in middle or late life, with the average age of diagnosis being 70-75 years old and by age 85, the risk reaches nearly 50%.
* 75% of patients are African American.
* Adults with a BMI of 25.0–29.9 in mid-life (ages 45–55) have a higher chance of developing Alzheimer's disease or dementia later in life (ages 65 and older). Adults with a BMI of more than 30.0 in mid-life have an even higher risk of developing Alzheimer's disease
* Smoking is responsible for 14% of AD cases worldwide. Smoking can increase the risk of AD in a number of ways
* Most Patients who are diagnosed have low physical activities
* Diet quality is a critical factor in managing Alzheimer's Disease (AD) and potentially reducing the risk of developing it
* Sleep quality is a crucial aspect of managing Alzheimer's Disease (AD) and can significantly impact cognitive function, overall health, and the progression of the disease
* 32% Percent of patients who have a family history of the disease have also developed it
* Cardiovascular disease is related to Alzheimer's disease because they share several risk factors and underlying mechanisms. Poor cardiovascular health, such as high blood pressure, high cholesterol, obesity, diabetes, and smoking, can increase the risk of developing Alzheimer's. 40% of the patients with Cardiovascular Disease have developed Alzheimer's
* 30% Patients with Diabetes are diagnosed with Alzheimer
* Depression is linked to an increased risk of developing Alzheimer's disease. it is considered a significant risk factor as 34% Patients who have Depression are diagnosed with Alzheimer
* 32% Patients with Head injury have Alzheimer, People who have had a head injury are more likely to develop Alzheimer's disease. Head injuries, especially severe or repeated ones, can damage brain cells and can increase the risk of cognitive decline and dementia, including Alzheimer's disease
* Hypertension is also a significant risk factor for Alzheimer's disease with 39% patients with Hypertension are diagnosed with the disease
* 63-65% Patients who have Presence of memory complaints also are Diagnosed with Alzheimer's Disease
* Patients with Behavioral Problems are 60% more likely to be Diagnosed with Alzheimer

# **5. Data Preprocessing**
"""

df

"""## **A. Data Cleaning**

No Data Cleaning is required as we have Concluded that in the EDA part

## **B. Feature Engineering**

As we have analyzed during EDA part, there are no Text Categorical Columns so no need of any Encoding, However, we do have Non-Binary Numerical Columns that needed to be scaled and also we need to make our Data more Gaussian like as well.

We will also need to create some new features to give more impact to Medical History columns and Symptoms columns as they are more impactful in predicting the disease

So first we will create some important features from Medical History and Symptoms columns and then we will create a Pipeline to do Imputing (just incase), Scaling and apply Power Transformer to make the data have more of a Normal Distribution
"""

df.head()

"""### **Feature Engineering on Medical History columns**

We can Create a composite score that sums up the presence of risk factors. For instance, a new feature that is the sum of FamilyHistoryAlzheimers, CardiovascularDisease, Diabetes, Depression, HeadInjury, and Hypertension.
"""

df['RiskFactorScore'] = df[['FamilyHistoryAlzheimers', 'CardiovascularDisease', 'Diabetes', 'Depression', 'HeadInjury', 'Hypertension']].sum(axis=1)
df.head()

"""We can create new features that represent the interaction between diabetes and cardiovascular disease.

This acts as an indicator that both conditions are present simultaneously. This new feature can help our model capture the combined effect of having both diabetes and cardiovascular disease.
"""

df['Diabetes_CVD'] = df['Diabetes'] * df['CardiovascularDisease']
df.head()

"""We can also create age buckets. Alzheimer's risk increases with age, so categorizing ages into groups like 60-70, 70-80, 80+, etc., might be useful.

For some reason i was getting NAN value below where age is 60 due to it being right-inclusive.

reason:

The issue where the AgeGroup column shows NaN for age 60 likely arises because the bin edges you specified do not include the lower bound of your age range. In pd.cut, the default behavior is to be right-inclusive (i.e., the right edge is included in the bin), but the left edge is not. Therefore, the age 60 is not being included in the first bin [60, 70).

To include 60 in the range, you can adjust the right parameter to False, making the bins left-inclusive.
"""

df['AgeGroup'] = pd.cut(df['Age'], bins=[60, 70, 80, 90, 100], labels=['60-70', '70-80', '80-90', '90+'], right=False)
df.head()

df['AgeGroup'].unique()

df[df['Age']==60]

"""Perfect, now there's no NAN values

We can also Create a feature that indicates if an individual has at least one risk factor. This can highlight patients who are exposed to any risk.
"""

df['HasAnyRiskFactor'] = df[['FamilyHistoryAlzheimers', 'CardiovascularDisease', 'Diabetes', 'Depression', 'HeadInjury', 'Hypertension']].apply(lambda x: 1 if x.sum() > 0 else 0, axis=1)
df.head()

"""### **Feature Engineering on Symptoms columns**

Given that our symptom columns are binary and the majority of diagnosed patients do not exhibit these symptoms, we will do some feature engineering to potentially enhance the model

We can Create a feature that counts the number of symptoms present for each patient. This can help capture the overall symptom burden
"""

df['SymptomCount'] = df[['Confusion', 'Disorientation', 'PersonalityChanges', 'DifficultyCompletingTasks', 'Forgetfulness']].sum(axis=1)
df.head()

"""We will also Create a binary feature that indicates whether any of the symptoms are present. This can highlight patients who have at least one symptom."""

df['HasAnySymptom'] = df[['Confusion', 'Disorientation', 'PersonalityChanges', 'DifficultyCompletingTasks', 'Forgetfulness']].apply(lambda x: 1 if x.sum() > 0 else 0, axis=1)
df.head()

df.shape

df

"""### **Encoding**"""

df = pd.get_dummies(df, columns=['AgeGroup'], drop_first=True).astype(int)
df

corr_with_target = df.corr()['Diagnosis'].sort_values(ascending=False)
plt.figure(figsize=(10, 6))
sns.barplot(x=corr_with_target.index, y=corr_with_target.values)
plt.xticks(rotation=90)
plt.title('Correlation of Features with Diagnosis')
plt.show()

"""### Now lets create a Pipeline to Scale and Transform all the Non-Binary Numerical columns"""

binary_col = []
for col in df.columns:
    if df[col].dtype in ["int64", "float64"]:
        unique_val = df[col].nunique()
        if unique_val == 2:
            binary_col.append(col)
df[binary_col]

nonbinary_col = []
for col in df.columns:
    if df[col].dtype in ["int64", "float64"]:
        unique_val = df[col].nunique()
        if unique_val > 2:
            nonbinary_col.append(col)
df[nonbinary_col]

binary_col.remove("Diagnosis")
len(binary_col)

df.shape

pipeline1 = Pipeline(steps=[
    ("impute", SimpleImputer(strategy="median")),
    ("power_transformer", PowerTransformer()),
    ("scaler", StandardScaler()),
])

pipeline2 = Pipeline(steps=[
    ("impute", SimpleImputer(strategy="median")),
])

transformer = ColumnTransformer(transformers=[
    ("pipeline1", pipeline1, nonbinary_col),
    ("pipeline2", pipeline2, binary_col)
])

transformed_data = transformer.fit_transform(df.drop(['Diagnosis'],axis=1))
transformed_data

transformed_data.shape

"""We got our Transformed data, now we will simply extract the Features names and Create a new Transformed DataFrame out of it"""

pipeline1_features_names = transformer.\
named_transformers_['pipeline1'].get_feature_names_out(nonbinary_col).tolist()

pipeline2_features_names = transformer.\
named_transformers_['pipeline2'].get_feature_names_out(binary_col).tolist()

features_names = pipeline1_features_names + pipeline2_features_names
features_names

pd.set_option('display.max_columns', 100)
transformed_df = pd.DataFrame(transformed_data, columns=features_names)
transformed_df[binary_col] = transformed_df[binary_col].astype(int)
transformed_df['Diagnosis'] = df['Diagnosis']
transformed_df

transformed_df.dtypes

transformed_df.isna().sum()

"""YAY!!! We are done doing Transformation on our Data! Now we can feed this Datas into a Machine Learning Model

Now we can move on to the Most Interesting part!

# **6. Machine Learning**

## **A. Data Splitting**
"""

X = transformed_df.drop(['Diagnosis'], axis=1)
y = transformed_df['Diagnosis']

X.shape

"""I wont do Validation split since we have very small data, that would just limit our Data even more"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
len(X_train), len(X_test)

y_train.value_counts()

y_test.value_counts()

"""We have very small dataset so thats the problem but anyways, now lets handle the Imbalance in the Dataset, I will use SMOTE

## **B. Oversampling using SMOTE**
"""

smote = SMOTE(sampling_strategy="minority")
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)
y_train_smote.value_counts()

"""## **C. Model Selection and Hyperparameters tuning**

We will use GridSearchCV and determine the Best Model with its Best hyper parameters and also Train an Artificial Neural Network (ANN) and see which Model or ANN performed the best then we will finalize that.

One of my fav things about ML is trying out different Models and Neural Network with different Hyperparameters and determine the best one, we will be doing exactly that with different Models, oh god, i just know its gonna take forever to Train for me due to my Potato Laptop 😢

We will check the score using the Cross Validation method as well as using the accuracy score and also check the ROC AUC score which is The ability to distinguish between classes.
"""

def main_gridsearchcv(model_params):
    scores = []
    kfold = StratifiedKFold(n_splits=5)
    for model, mp in model_params.items():
        grid = GridSearchCV(mp['model'], mp['params'], cv=kfold, return_train_score=False)
        grid.fit(X_train_smote, y_train_smote)

        # Predictions
        y_pred = grid.predict(X_test)

        # Calculate metrics
        roc_auc = roc_auc_score(y_test, y_pred)
        accuracy = accuracy_score(y_test, y_pred)

        scores.append({
            "model" : model,
            "best_estimator" : grid.best_estimator_,
            "best_score" : grid.best_score_,
            "best_params" : grid.best_params_,
            "roc_auc": roc_auc,
            "accuracy": accuracy
        })
    resultdf = pd.DataFrame(scores, columns=["model", "best_estimator", "best_score", "best_params", "roc_auc", "accuracy"])
    return resultdf

"""Now lets create a Dictionary of some Selected Models with their Hyperparameters"""

model_params = {
    "svm" : {
        "model" : SVC(),
        "params" : {
            "C" : [1, 10, 20, 30, 50, 100],
            "kernel" : ['linear', 'poly', 'rbf'],
             "gamma" : ["auto", "scale"]
        }
    },

    "bagging_decision_tree": {
        "model": BaggingClassifier(estimator=DecisionTreeClassifier()),
        "params": {
            "n_estimators": [5, 10, 20, 50, 100, 150, 200, 270],
            "max_samples": [0.7, 0.8, 0.9],
            "estimator__criterion": ['gini', 'entropy']
        }
    },

    'random forest' : {
        "model" : RandomForestClassifier(),
        "params" : {
            "n_estimators" : [5, 10, 20, 50, 100, 150, 200],
            'criterion' : ['gini', 'entropy'],
            'max_depth' : [None, 1,2,3]
        }
    },


    "knn" : {
        "model" : KNeighborsClassifier(),
        "params" : {
            "n_neighbors" : range(1, 30)
        }
    },

}

main_gridsearchcv(model_params)

"""## **D. Artificial Neural Network (ANN)**

However i would like to Train an Artificial Neural Network (ANN) as well just for fun and check the Performance.
"""

X_train_smote.shape

nn = Sequential([
    Input(shape=(40,)),
    Dense(units=40, activation="relu"),
    Dropout(0.3),
    Dense(units=64, activation="relu"),
#     Dropout(0.3),
    Dense(units=1, activation="sigmoid"),
])

nn.compile(optimizer="adam",
          loss='binary_crossentropy',
          metrics=['accuracy'])

nn.fit(X_train_smote, y_train_smote, epochs=100)

print(f"Evaluation score : {nn.evaluate(X_test, y_test)}")
y_pred = nn.predict(X_test)
y_pred = [1 if i > 0.5 else 0 for i in y_pred]
print(f"ROC_AUC score : {roc_auc_score(y_test, y_pred)}")
print(f"Accuracy : {accuracy_score(y_test, y_pred)}")

"""## **E. Training and Predicting using Final Model**"""

kittycat = CatBoostClassifier(verbose=False)
kittycat.fit(X_train_smote, y_train_smote)
kittycat.score(X_test, y_test)

final_predict = kittycat.predict(X_test)
roc_auc_score(y_test, final_predict)

"""## **F. Evaluation on Final Model**"""

print(classification_report(y_test, final_predict))

cm = confusion_matrix(y_test, final_predict)
plt.figure(figsize=(10, 7))
plt.title("Truth vs Predicted")
sns.heatmap(cm, annot=True, fmt="d")
plt.xlabel("Predicted Values")
plt.ylabel("Truth Values")

len(final_predict), np.sum(final_predict != y_test)

"""Only 21 Misclassification out of 430 samples! Which is very IMPRESSIVE!

# **7. Save Model**
"""

import pickle

# Save the model to a pickle file
with open('catboost_alzheimer_model.pickle', 'wb') as f:
    pickle.dump(kittycat, f)
print("Model Succesfully saved.")

import json
columns = {
    'data_columns' : [col.lower() for col in X.columns]
}
with open("columns.json","w") as f:
    f.write(json.dumps(columns))

transformed_df.to_csv("preprocessd_alzheimer.csv")